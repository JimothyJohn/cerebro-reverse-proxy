## Problem

It’s extremely costly and cumbersome to utilize vision perception models for niche use cases at the edge and I hope to make this easier, faster, cheaper, and more accessible by offering an efficient model accelerated in a cloud deployment. There’s a deeper problem here, but this is the technical one

## Goal

Create a reverse proxy using [AWS Serverless Application Model](https://aws.amazon.com/serverless/sam/) for Cerebro, a Replicate API endpoint at https://replicate.com/jimothyjohn/phi3-vision-instruct that returns a text response.

## Stakeholders

The designer of this architecture is an automation engineer specializing in deploying AI endpoints for scale. They have experience with Debian, Docker, CUDA, Cog, and Python. The user of the endpoint will be an enterprise user trying to gain insight into images by prompting with hints about what is in the image in an attempt to get a structured output with key details about what they need to know.

## Requirements

- Make it clear in all files that they are AI-generated by the current model being used (o1, Sonnet, 4o, Mistral, Gemini, Phi, etc.) so the user knows to check for errors.
- The program should be easy to follow and well-commented.
- The program will provide detailed logging to help troubleshoot during development.
- When providing code provide the entire program file, not just snippets.
- Make it as generalizable as possible in case I want to deploy other methods.
- Keep all of the program explanations inside of the files themselves as comments.
- Utilize the OpenAI API standard practices for the user of the Lambda endpoint
- Use containerization whenever possible to improve modularity
- The user is expected to provide an API key in their request
- Recommend the most stable versions possible as of your knowledge cutoff date for maximum alignment.
- Keep the API as basic as possible to make it easier for the user to get started and follow closely with OpenAPI to keep things familiar.

## Error Handling / Testing

- Ensure the user to provide an API key in their request - Create an error when none is provided or when Replicate says the one being provided is invalid.
- Make sure an image is sent to the API
- Make sure text is also provided with the image
- Make sure the endpoint format is as expected
- Make sure input format is as expected
- More stuff that would be typical in an API

## Outputs (in order of creation)

### requirements.txt

A file of the library dependencies

```
PyYAML
pytest
black
boto3
requests
```

### app.py

A Python program that functions as the main Lambda program. Here’s an example using a detection model that runs on Ultralytics’ YOLOv8 on AWS Lambda itself rather than an external Replicate API endpoint:

```python
# app.py
# Runs a detection model using Ultralytics’ YOLOv8 on AWS Lambda
# Human-generated using example: https://github.com/trainyolo/YOLOv8-aws-lambda/blob/main/lambda-codebase/app.py

import os
import json
import base64
from io import BytesIO
from typing import Dict, List, Any
from PIL import Image
import yaml
from yolov8_onnx import YOLOv8

class CocoClassMapper:
    def __init__(self, coco_yaml_path: str):
        self.class_id_to_name = self._load_class_names(coco_yaml_path)

    @staticmethod
    def _load_class_names(yaml_path: str) -> Dict[int, str]:
        """
        Loads the class ID to name mapping from the coco.yaml file.

        :param yaml_path: Path to the coco.yaml file.
        :return: A dictionary mapping class IDs to their names.
        """
        with open(yaml_path, "r") as file:
            data = yaml.safe_load(file)
            return {int(k): v for k, v in data["names"].items()}

    def map_class_names(self, detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Maps class IDs in the detections to their corresponding class names.

        :param detections: A list of detection dictionaries.
        :return: The enriched list of detections with class names.
        """
        for detection in detections:
            class_id = detection.get("class_id")
            detection["class_name"] = self.class_id_to_name.get(class_id, "Unknown")
        return detections

yolov8_detector = YOLOv8(
    os.path.join(os.path.dirname(os.path.realpath(__file__)), "models", "yolov8n.onnx")
)

mapper = CocoClassMapper(
    os.path.join(os.path.dirname(os.path.realpath(__file__)), "coco.yaml")
)

def detect(body: Dict[str, Any], class_mapper: CocoClassMapper) -> Dict[str, Any]:
    """
    Perform object detection on an image and return detections with class names.

    :param body: A dictionary containing the image and detection parameters.
    :param class_mapper: An instance of CocoClassMapper to map class IDs to names.
    :return: Detections with class names.
    """
    # Get parameters
    img_b64 = body["image"]
    SIZE = 640
    conf_thres = body.get("conf_thres", 0.7)
    iou_thres = body.get("iou_thres", 0.5)

    # Open and resize image
    img = Image.open(BytesIO(base64.b64decode(img_b64.encode("ascii"))))
    img_resized = img.resize((SIZE, SIZE))

    # Infer result
    detections = yolov8_detector(
        img_resized, size=SIZE, conf_thres=conf_thres, iou_thres=iou_thres
    )

    # Map class IDs to names
    detections_with_names = class_mapper.map_class_names(detections)

    return detections_with_names

def lambda_handler(event, context):
    """Sample pure Lambda function

    Parameters
    ----------
    event: dict, required
        API Gateway Lambda Proxy Input Format

        Event doc: https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format

    context: object, required
        Lambda Context runtime methods and attributes

        Context doc: https://docs.aws.amazon.com/lambda/latest/dg/python-context-object.html

    Returns
    ------
    API Gateway Lambda Proxy Output Format: dict

        Return doc: https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html
    """

    # try:
    #     ip = requests.get("http://checkip.amazonaws.com/")
    # except requests.RequestException as e:
    #     # Send some context about this error to Lambda Logs
    #     print(e)

    #     raise e

    # Check if the incoming request is a POST request
    if event["httpMethod"] != "POST":
        return {
            "statusCode": 400,
            "body": json.dumps(
                {"message": "Error: This endpoint only accepts POST requests."}
            ),
        }

    try:
        # Assuming detect is a function you've defined to process the incoming request
        detections = detect(json.loads(event["body"]), mapper)
        return {"statusCode": 200, "body": json.dumps({"detections": detections})}
    except Exception as e:
        # Handle other exceptions here as needed
        print(e)
        return {"statusCode": 500, "body": json.dumps({"message": str(e)})}

```

[tests.py](http://tests.py) - Basic unit tests built using the pytest framework

### template.yaml

A configuration file that will serve as the configuration file for the serverless application model. I want to use the settings:

- Runtime: Python
- Memory Size 2048
    - I want to reduce this as much as possible in the future
- Ephemeral Storage Size: 1024
    - I want to reduce this as much as possible in the future

Here’s an example using a detection model that runs on Ultralytics’ YOLOv8 on AWS Lambda itself rather than an external Replicate API endpoint:

```yaml
# template.yaml 
# AWS SAM configuration file for a detection model that runs on Ultralytics’ YOLOv8 on AWS Lambda
# Human-generated using examples
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: A lightweight, general purpose object detection endpoint.

# More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst
Globals:
  Function:
    Timeout: 10
    MemorySize: 1024
    Tracing: Active
    # You can add LoggingConfig parameters such as the Logformat, Log Group, and SystemLogLevel or ApplicationLogLevel. Learn more here https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-resource-function.html#sam-function-loggingconfig.
    LoggingConfig:
      LogFormat: JSON
  Api:
    TracingEnabled: true
Resources:
  CerebroFunction:
    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction
    Properties:
      CodeUri: cerebro/ 
      Handler: app.lambda_handler
      Runtime: python3.11
      Architectures:
      - x86_64
      EphemeralStorage:
        Size: 512
      EventInvokeConfig:
        MaximumEventAgeInSeconds: 21600
        MaximumRetryAttempts: 2
      Policies:
        - Statement:
            - Effect: Allow
              Action:
                - logs:CreateLogGroup
              Resource: !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*
            - Effect: Allow
              Action:
                - logs:CreateLogStream
                - logs:PutLogEvents
              Resource:
                - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/cerebro:*
      SnapStart:
        ApplyOn: None
      Events:
        DetectGet:
            Type: Api
            Properties:
              Path: /v1/completions
              Method: get
        DetectPost:
          Type: Api # More info about API Event Source: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#api
          Properties:
            Path: /v1/completions
            Method: post

  ApplicationResourceGroup:
    Type: AWS::ResourceGroups::Group
    Properties:
      Name:
        Fn::Sub: ApplicationInsights-SAM-${AWS::StackName}
      ResourceQuery:
        Type: CLOUDFORMATION_STACK_1_0
  ApplicationInsightsMonitoring:
    Type: AWS::ApplicationInsights::Application
    Properties:
      ResourceGroupName:
        Ref: ApplicationResourceGroup
      AutoConfigurationEnabled: 'true'
Outputs:
  # ServerlessRestApi is an implicit API created out of Events key under Serverless::Function
  # Find out more about other implicit resources you can reference within SAM
  # https://github.com/awslabs/serverless-application-model/blob/master/docs/internals/generated_resources.rst#api
  CerebroApi:
    Description: API Gateway endpoint URL for Prod stage for Cerebro function
    Value: !Sub "https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/detect/"
  CerebroFunction:
    Description: Cerebro Lambda Function ARN
    Value: !GetAtt CerebroFunction.Arn
  CerebroFunctionIamRole:
    Description: Implicit IAM Role created for Cerebro function
    Value: !GetAtt CerebroFunctionRole.Arn
```

### Quickstart.sh

A command that can be run from the root that creates the environment for this project, creating all files/folders, installing dependencies, and running deployment commands (if required). It will include a helpful CLI that guides the user through the whole process and take input arguments that would likely be necessary.

```bash
#!/usr/bin/env bash

set -o errexit
set -o nounset
set -o pipefail
set -e

if [[ "${TRACE-0}" == "1" ]]; then
    set -o xtrace
fi

# Function to display help
help_function() {
    cat <<EOF

Usage: $(basename "$0") [-h] [--lib-dir LIB_DIR]

Options:
  -h, --help       Display this help message and exit
  --lib-dir        Optional path to the lib/ directory
  -f, --function   Name of the lambda function

EOF
}

# Function to check if a command exists
command_exists() {
    type "$1" &> /dev/null
}

# Check required dependencies
check_dependencies() {
    if ! command_exists docker; then
        cat <<EOF

Docker is not installed. Install it using:

curl -fsSL https://get.docker.com -o get-docker.sh && \\
    sudo sh get-docker.sh

EOF
        exit 1
    fi

    if ! command_exists sam; then
        cat <<EOF

AWS SAM CLI is not installed. Install it with:

wget https://github.com/aws/aws-sam-cli/releases/latest/download/aws-sam-cli-linux-x86_64.zip && \\
    unzip aws-sam-cli-linux-x86_64.zip -d sam-installation && \\
    sudo ./sam-installation/install && \\
    rm -rf aws-sam-cli-linux-x86_64.zip sam-installation

EOF
        exit 1
    fi

    if ! command_exists pip; then
        cat <<EOF

Pip is not installed. Install it with:

Linux:

sudo apt install python3-pip

MacOS:

brew install python3-pip

EOF
        exit 1
    fi
}

# Main function of the script
main() {
    local FUNCTION="cerebro"

    # Parse arguments
    while [[ "$#" -gt 0 ]]; do
        case "$1" in
            -h|--help)
                help_function
                exit 0
                ;;
            -f|--function)
                FUNCTION="$2"
                shift 2
                ;;
            *)
                echo "Unknown parameter passed: $1"
                help_function
                exit 1
                ;;
        esac
    done

    # Check for required dependencies
    check_dependencies

    # Install virtual environment 
    echo "Activating environment..."
    python3 -m venv venv && \
        source venv/bin/activate && \
        pip install -q --upgrade pip && \
        pip install -q -r requirements.txt

    # Deploy updates to AWS
    echo "Updating endpoint..."
    black -q cerebro/ && \
        sam validate && \
        sam build && \
        PYTHONPATH="$(pwd)"/cerebro pytest -q tests/unit && \
        deactivate && \
        sam sync --stack-name sam-app --watch
}

main "$@"

```

index.html and style.css - An HTML, more webpage-like and graphical interpretation of the  README that will be used in conjunction with Github Pages along with CSS for readability and formatting.

.gitignore - A .gitignore file suitable for this type of Python-Lambda deployment

devcontainer.json - A fully-featured development container file for VS Code that makes development instantaneous and lends itself well to Github Codespaces for cloud development

[PROMPT.](http://PROMPT.md)txt - Saves this prompt as a plaintext file in a more readable and easily applicable format.

### README.md

A markdown file that elegantly explains what this repository does without providing unnecessary details likely not relevant to the common user.

```markdown
# Cerebro - Dawn of Perception

This repository provides a setup for deploying Microsoft's multi-modal Phi-3 model using ONNX Runtime with CUDA acceleration. The goal is to create a Replicate A100 endpoint for efficient cloud deployment, making it easier, faster, and more accessible to utilize vision perception models for niche use cases.

## Features

- **CUDA Acceleration**: Leverages CUDA for faster inference on GPUs.
- **Easy Deployment**: Configurations are provided for quick setup using VS Code or GitHub Codespaces.

## Getting Started

### Prerequisites

- NVIDIA GPU with CUDA support
- Docker installed (for devcontainer)
- VS Code with Remote Containers extension (optional)

### Setup Instructions

1. **Clone the Repository**

   ```bash
   git clone https://github.com/JimothyJohn/cerebro.git
   cd phi-3-deployment
   ```

2. **Using Dev Container**
    
    * Open the repository in VS Code.
    
    * When prompted, "Reopen in Container" to use the devcontainer setup.
    
    * Alternatively, you can use GitHub Codespaces for a cloud-based development environment.

3. **Install Dependencies**

    * If not using the devcontainer, create a virtual environment and install dependencies:

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

4. **Run Tests**

    * Ensure everything is set up correctly by running the unit tests:

```bash
pytest tests.py
```

5. **Running Inference**

    * You can run the predict.py script directly or set it up as part of a Cog deployment.

## Usage

The predict.py script accepts image URLs and a text prompt to generate a response using the Phi-3 model.

Example:

```py
from predict import Predictor

predictor = Predictor()
predictor.setup()
response = predictor.predict(
    image_urls="https://example.com/image1.jpg, https://example.com/image2.jpg",
    prompt="Summarize the content of these images.",
    max_new_tokens=100,
    temperature=0.5,
    do_sample=True
)

print(response)
```

### Notes

* AI-Generated Code: Most of the code in this repository is AI-generated and should be reviewed for correctness and compatibility.

* Model Files: The model files are downloaded automatically if not present in the model-cache directory.

### Contributing

Contributions are welcome! Please submit a pull request or open an issue for any changes or suggestions.

### License - MIT

```

## Examples

### predict.py

The code for the endpoint that you’ll be interfacing too.

```python
# predict.py
# Prediction interface for Replicate endpoint
# https://github.com/replicate/cog/blob/main/docs/python.md
"""
This program is for Phi-3.5-vision-instruct, the multi-modal generation of Phi models.
AI-generated code, please review for correctness.
"""

from cog import BasePredictor, Input
import os
import time
import torch
from PIL import Image
from transformers import AutoModelForCausalLM, AutoProcessor
import requests
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

MODEL_NAME = "microsoft/Phi-3.5-vision-instruct"
MODEL_CACHE = "/model-cache"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def is_ampere_or_newer():
    if torch.cuda.is_available():
        capability = torch.cuda.get_device_capability()
        return capability[0] >= 8  # Ampere is compute capability 8.0+
    return False

attn_implementation = "flash_attention_2" if is_ampere_or_newer() else "eager" 

class Predictor(BasePredictor):
    def setup(self):
        """Load the model into memory to make running multiple predictions efficient"""
        start_time = time.time()
        logger.info("Setting up the model...")
        try:
            # Load the model from the cache directory
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_CACHE,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                _attn_implementation=attn_implementation  # or 'eager' if flash_attn not installed
            )
            # For best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.
            self.processor = AutoProcessor.from_pretrained(
                MODEL_CACHE,
                trust_remote_code=True,
                num_crops=4 # multi-frame
                # num_crops=16 # single-frame, results in torch.OutOfMemoryError: CUDA out of memory
            )
            logger.info(f"Model loaded in {time.time() - start_time:.2f} seconds")
        except Exception as e:
            logger.error(f"Error during model setup: {e}")
            raise e

    @torch.inference_mode()
    def predict(
        self,
        image_urls: str = Input(description="Comma-separated URLs of images"),
        prompt: str = Input(description="Input prompt"),
        max_new_tokens: int = Input(
            description="Max new tokens", default=1000, ge=1, le=2048
        ),
        temperature: float = Input(
            description="Temperature for generation", default=0.7, ge=0.0, le=1.0
        ),
        do_sample: bool = Input(
            description="Whether or not to use sampling; use greedy decoding otherwise.", default=True
        ),
    ) -> str:
        """Run a single prediction on the model"""
        try:
            # Process images
            images = []
            placeholder = ""
            image_url_list = [url.strip() for url in image_urls.split(",")]
            for idx, url in enumerate(image_url_list):
                response = requests.get(url, stream=True)
                if response.status_code == 200:
                    image = Image.open(response.raw).convert("RGB")
                    images.append(image)
                    placeholder += f"<|image_{idx+1}|>\n"
                else:
                    logger.warning(f"Failed to retrieve image from URL: {url}")
            if not images:
                raise ValueError("No valid images were provided.")

            # Prepare messages
            messages = [
                {"role": "user", "content": placeholder + prompt},
            ]

            # Prepare prompt
            prepared_prompt = self.processor.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            # Prepare inputs
            inputs = self.processor(
                prepared_prompt,
                images=images,
                return_tensors="pt"
            ).to(DEVICE)

            # Generation arguments
            generation_args = {
                "max_new_tokens": max_new_tokens,
                "temperature": temperature,
                "do_sample": do_sample,
                "eos_token_id": self.processor.tokenizer.eos_token_id,
            }

            # Generate output
            outputs = self.model.generate(**inputs, **generation_args)

            # Remove input tokens
            generated_tokens = outputs[:, inputs['input_ids'].shape[1]:]

            # Decode response
            response = self.processor.batch_decode(
                generated_tokens,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]

            return response

        except Exception as e:
            logger.error(f"Error during prediction: {e}")
            raise e

```

### replicate.py

Sample code how to use the Replicate endpoint.

```python
import replicate

output = replicate.run(
    "jimothyjohn/phi3-vision-instruct:eb47203b095603b4189c941667c3e21b7aba6694af845dae9a889a348581ed17",
    input={
        "prompt": "How many people are in this image/",
        "do_sample": True,
        "image_urls": "https://github.com/JimothyJohn/cerebro/blob/master/data/images/zidane.jpg?raw=true",
        "temperature": 0.7,
        "max_new_tokens": 1000
    }
)
print(output)
```

### events.json

```json
{
  "body": "{\"message\": \"detect world\"}",
  "resource": "/detect",
  "path": "/detect",
  "httpMethod": "POST",
  "isBase64Encoded": false,
  "queryStringParameters": {
    "foo": "bar"
  },
  "pathParameters": {
    "proxy": "/path/to/resource"
  },
  "stageVariables": {
    "baz": "qux"
  },
  "headers": {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Encoding": "gzip, deflate, sdch",
    "Accept-Language": "en-US,en;q=0.8",
    "Cache-Control": "max-age=0",
    "CloudFront-Forwarded-Proto": "https",
    "CloudFront-Is-Desktop-Viewer": "true",
    "CloudFront-Is-Mobile-Viewer": "false",
    "CloudFront-Is-SmartTV-Viewer": "false",
    "CloudFront-Is-Tablet-Viewer": "false",
    "CloudFront-Viewer-Country": "US",
    "Host": "1234567890.execute-api.us-east-1.amazonaws.com",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Custom User Agent String",
    "Via": "1.1 08f323deadbeefa7af34d5feb414ce27.cloudfront.net (CloudFront)",
    "X-Amz-Cf-Id": "cDehVQoZnx43VYQb9j2-nvCh-9z396Uhbp027Y2JvkCPNLmGJHqlaA==",
    "X-Forwarded-For": "127.0.0.1, 127.0.0.2",
    "X-Forwarded-Port": "443",
    "X-Forwarded-Proto": "https"
  },
  "requestContext": {
    "accountId": "123456789012",
    "resourceId": "123456",
    "stage": "prod",
    "requestId": "c6af9ac6-7b61-11e6-9a41-93e8deadbeef",
    "requestTime": "09/Apr/2015:12:34:56 +0000",
    "requestTimeEpoch": 1428582896000,
    "identity": {
      "cognitoIdentityPoolId": null,
      "accountId": null,
      "cognitoIdentityId": null,
      "caller": null,
      "accessKey": null,
      "sourceIp": "127.0.0.1",
      "cognitoAuthenticationType": null,
      "cognitoAuthenticationProvider": null,
      "userArn": null,
      "userAgent": "Custom User Agent String",
      "user": null
    },
    "path": "/prod/detect",
    "resourcePath": "/detect",
    "httpMethod": "POST",
    "apiId": "1234567890",
    "protocol": "HTTP/1.1"
  }
}
```

## Follow-up

After all of the above outputs have been created:

- Provide step by step instructions showing how to deploy the code as a new Github repository using bash in a Unix-like terminal.
- Provide suggestions for additional functionality and enhancements after the outputs
- Recommend adjustments to the prompt to better refine results
- Point out areas of ambiguity or confusion where clarification is needed